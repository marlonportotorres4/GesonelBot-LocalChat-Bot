# GesonelBot - Chatbot Local para Documentos

![GesonelBot Logo](docs/images/gesonelbot_logo.png)

> Um assistente de IA local que responde a perguntas com base nos seus documentos

## üìñ Sobre

GesonelBot √© um chatbot que utiliza processamento de linguagem natural e recupera√ß√£o de informa√ß√µes para responder a perguntas com base em documentos locais. Diferente de outras solu√ß√µes, o GesonelBot funciona 100% localmente em seu computador, sem enviar seus dados para servi√ßos externos.

## üì∏ Screenshots

### Interface de Chat
![Tela Principal](docs/images/telaprincipal.png)

### Upload de Documentos
![Tela do Chat](docs/images/teladochat.png)

### Upload de Documentos
![Tela Final](docs/images/telafinal.png)


## ‚ú® Funcionalidades

- üè† **Totalmente Local**: Todo o processamento ocorre em sua m√°quina
- üìÑ **Suporte a M√∫ltiplos Formatos**: PDF, DOCX, TXT e mais
- üîç **Busca Sem√¢ntica**: Encontra informa√ß√µes relevantes mesmo quando n√£o h√° correspond√™ncia exata
- üß† **Modelo TinyLlama**: Utiliza um modelo de linguagem eficiente que funciona em hardware comum
- üîÑ **Processamento em Lote**: Processe m√∫ltiplos documentos de uma vez
- üåê **Interface Web**: Interface amig√°vel baseada em Gradio

## üñ•Ô∏è Requisitos

- Python 3.8 ou superior
- Windows, MacOS ou Linux
- M√≠nimo de 4GB de RAM (8GB recomendado)
- Aproximadamente 2GB de espa√ßo em disco para o modelo

## üöÄ Instala√ß√£o

### M√©todo R√°pido (Windows)

1. Clone o reposit√≥rio:
   ```
   git clone https://github.com/seuusuario/GesonelBot-LocalChat-Bot.git
   cd GesonelBot-LocalChat-Bot
   ```

2. Execute o script de configura√ß√£o:
   ```
   scripts\setup.bat
   ```

3. Inicie o aplicativo:
   ```
   scripts\executar.bat
   ```

### Instala√ß√£o Manual

1. Clone o reposit√≥rio:
   ```
   git clone https://github.com/seuusuario/GesonelBot-LocalChat-Bot.git
   cd GesonelBot-LocalChat-Bot
   ```

2. Crie um ambiente virtual:
   ```
   python -m venv venv
   ```

3. Ative o ambiente virtual:
   - Windows: `venv\Scripts\activate`
   - Mac/Linux: `source venv/bin/activate`

4. Instale as depend√™ncias:
   ```
   pip install -r requirements.txt
   ```

5. Copie o arquivo de configura√ß√£o:
   ```
   cp env.example .env
   ```

6. Inicie o aplicativo:
   ```
   python gesonelbot.py
   ```

## üìù Uso

1. Acesse a interface web em `http://localhost:7860`
2. Na aba "Upload de Documentos", carregue seus arquivos
3. Clique em "Processar Documentos"
4. V√° para a aba "Chat" e comece a fazer perguntas sobre seus documentos

## üîß Configura√ß√£o

O comportamento do GesonelBot pode ser personalizado atrav√©s do arquivo `.env`:

- `LOCAL_MODEL_NAME`: O modelo Hugging Face a ser utilizado (padr√£o: TinyLlama/TinyLlama-1.1B-Chat-v1.0)
- `CHUNK_SIZE`: Tamanho dos fragmentos de texto para processamento (padr√£o: 1000)
- `CHUNK_OVERLAP`: Sobreposi√ß√£o entre fragmentos (padr√£o: 200)
- `QA_MAX_TOKENS`: N√∫mero m√°ximo de tokens na resposta (padr√£o: 512)
- `QA_TEMPERATURE`: Temperatura para gera√ß√£o de resposta (padr√£o: 0.7)

## üõ†Ô∏è Arquitetura

GesonelBot utiliza uma arquitetura moderna para processamento de documentos e resposta a perguntas:

1. **Processamento de Documentos**: Os documentos s√£o carregados, divididos em chunks e convertidos em embeddings
2. **Armazenamento Vetorial**: Os embeddings s√£o armazenados em um banco de dados ChromaDB local
3. **Recupera√ß√£o de Informa√ß√µes**: Quando uma pergunta √© feita, o sistema recupera os chunks mais relevantes
4. **Gera√ß√£o de Resposta**: O modelo TinyLlama gera respostas com base nos chunks recuperados e na pergunta do usu√°rio

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas! Por favor, sinta-se √† vontade para enviar pull requests ou abrir issues.

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a [MIT License](LICENSE).

## üôè Agradecimentos

- [LangChain](https://github.com/langchain-ai/langchain) pelo framework de processamento de documentos
- [Hugging Face](https://huggingface.co) pela biblioteca Transformers e modelos
- [TinyLlama](https://github.com/jzhang38/TinyLlama) pelo modelo eficiente de linguagem
- [Gradio](https://github.com/gradio-app/gradio) pela interface web
