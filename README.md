
# GesonelBot ğŸ¤– (Em desenvolvimento)

Um chatbot com IA para responder perguntas com base em documentos locais.

## ğŸ“– Sobre

GesonelBot Ã© um chatbot que utiliza processamento de linguagem natural e recuperaÃ§Ã£o de informaÃ§Ãµes para responder a perguntas com base em documentos locais. Diferente de outras soluÃ§Ãµes, o GesonelBot funciona 100% localmente em seu computador, sem enviar seus dados para serviÃ§os externos.

## ğŸ“¸ Screenshots

### Interface de Chat
![Tela Principal](docs/images/telaprincipal.png)

### Upload de Documentos
![Tela do Chat](docs/images/teladochat.png)

### Upload de Documentos
![Tela Final](docs/images/telafinal.png)


## âœ¨ Funcionalidades

- ğŸ  **Totalmente Local**: Todo o processamento ocorre em sua mÃ¡quina
- ğŸ“„ **Suporte a MÃºltiplos Formatos**: PDF, DOCX, TXT e mais
- ğŸ” **Busca SemÃ¢ntica**: Encontra informaÃ§Ãµes relevantes mesmo quando nÃ£o hÃ¡ correspondÃªncia exata
- ğŸ§  **Modelo TinyLlama**: Utiliza um modelo de linguagem eficiente que funciona em hardware comum
- ğŸ”„ **Processamento em Lote**: Processe mÃºltiplos documentos de uma vez
- ğŸŒ **Interface Web**: Interface amigÃ¡vel baseada em Gradio

## ğŸ–¥ï¸ Requisitos

- Python 3.8 ou superior
- Windows, MacOS ou Linux
- MÃ­nimo de 4GB de RAM (8GB recomendado)
- Aproximadamente 2GB de espaÃ§o em disco para o modelo

## ğŸš€ InstalaÃ§Ã£o

### MÃ©todo RÃ¡pido (Windows)

1. Clone o repositÃ³rio:
   ```
   git clone https://github.com/seuusuario/GesonelBot-LocalChat-Bot.git
   cd GesonelBot-LocalChat-Bot
   ```

2. Execute o script de configuraÃ§Ã£o:
   ```
   scripts\setup.bat
   ```

3. Inicie o aplicativo:
   ```
   scripts\executar.bat
   ```

### InstalaÃ§Ã£o Manual

1. Clone o repositÃ³rio:
   ```
   git clone https://github.com/seuusuario/GesonelBot-LocalChat-Bot.git
   cd GesonelBot-LocalChat-Bot
   ```

2. Crie um ambiente virtual:
   ```
   python -m venv venv
   ```

3. Ative o ambiente virtual:
   - Windows: `venv\Scripts\activate`
   - Mac/Linux: `source venv/bin/activate`

4. Instale as dependÃªncias:
   ```
   pip install -r requirements.txt
   ```

5. Copie o arquivo de configuraÃ§Ã£o:
   ```
   cp env.example .env
   ```

6. Inicie o aplicativo:
   ```
   scripts\executar.bat
   ```

## ğŸ“ Uso

1. Acesse a interface web em `http://localhost:7860`
2. Na aba "Upload de Documentos", carregue seus arquivos
3. Clique em "Processar Documentos"
4. VÃ¡ para a aba "Chat" e comece a fazer perguntas sobre seus documentos

## ğŸ”§ ConfiguraÃ§Ã£o

O comportamento do GesonelBot pode ser personalizado atravÃ©s do arquivo `.env`:

- `LOCAL_MODEL_NAME`: O modelo Hugging Face a ser utilizado (padrÃ£o: TinyLlama/TinyLlama-1.1B-Chat-v1.0)
- `CHUNK_SIZE`: Tamanho dos fragmentos de texto para processamento (padrÃ£o: 1000)
- `CHUNK_OVERLAP`: SobreposiÃ§Ã£o entre fragmentos (padrÃ£o: 200)
- `QA_MAX_TOKENS`: NÃºmero mÃ¡ximo de tokens na resposta (padrÃ£o: 512)
- `QA_TEMPERATURE`: Temperatura para geraÃ§Ã£o de resposta (padrÃ£o: 0.7)

## ğŸ› ï¸ Arquitetura

GesonelBot utiliza uma arquitetura moderna para processamento de documentos e resposta a perguntas:

1. **Processamento de Documentos**: Os documentos sÃ£o carregados, divididos em chunks e convertidos em embeddings
2. **Armazenamento Vetorial**: Os embeddings sÃ£o armazenados em um banco de dados ChromaDB local
3. **RecuperaÃ§Ã£o de InformaÃ§Ãµes**: Quando uma pergunta Ã© feita, o sistema recupera os chunks mais relevantes
4. **GeraÃ§Ã£o de Resposta**: O modelo TinyLlama gera respostas com base nos chunks recuperados e na pergunta do usuÃ¡rio

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a [MIT License](LICENSE).

## ğŸ™ Agradecimentos

- [LangChain](https://github.com/langchain-ai/langchain) pelo framework de processamento de documentos
- [Hugging Face](https://huggingface.co) pela biblioteca Transformers e modelos
- [TinyLlama](https://github.com/jzhang38/TinyLlama) pelo modelo eficiente de linguagem
- [Gradio](https://github.com/gradio-app/gradio) pela interface web
